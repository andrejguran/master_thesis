
@article{huang_multimodal_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.04732},
  primaryClass = {cs, stat},
  title = {Multimodal {{Unsupervised Image}}-to-{{Image Translation}}},
  url = {http://arxiv.org/abs/1804.04732},
  abstract = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT.},
  urldate = {2018-04-30},
  date = {2018-04-12},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
  file = {/Users/sonynka/Zotero/storage/SXYG63DR/Huang et al. - 2018 - Multimodal Unsupervised Image-to-Image Translation.pdf;/Users/sonynka/Zotero/storage/BJGS7X6V/1804.html},
  annotation = {MUNIT

nvidia labs}
}

@article{zhu_toward_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.11586},
  primaryClass = {cs, stat},
  title = {Toward {{Multimodal Image}}-to-{{Image Translation}}},
  url = {http://arxiv.org/abs/1711.11586},
  abstract = {Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a $\backslash$emph\{distribution\} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.},
  urldate = {2018-04-30},
  date = {2017-11-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Graphics},
  author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A. and Wang, Oliver and Shechtman, Eli},
  file = {/Users/sonynka/Zotero/storage/PZCA7WN6/Zhu et al. - 2017 - Toward Multimodal Image-to-Image Translation.pdf;/Users/sonynka/Zotero/storage/ZYT7IV2E/1711.html},
  annotation = {BiCycleGAN

Comment: NIPS 2017 Final paper. v2 adds implementation details; Website: https://junyanz.github.io/BicycleGAN/

UC Berkeley and Adobe Research}
}

@article{choi_stargan_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.09020},
  primaryClass = {cs},
  title = {{{StarGAN}}: {{Unified Generative Adversarial Networks}} for {{Multi}}-{{Domain Image}}-to-{{Image Translation}}},
  url = {http://arxiv.org/abs/1711.09020},
  shorttitle = {{{StarGAN}}},
  abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
  urldate = {2018-04-30},
  date = {2017-11-24},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  file = {/Users/sonynka/Zotero/storage/XSNV9EXA/Choi et al. - 2017 - StarGAN Unified Generative Adversarial Networks f.pdf;/Users/sonynka/Zotero/storage/7WYL85FA/1711.html}
}

@incollection{lample_fader_2017,
  title = {Fader {{Networks}}:{{Manipulating Images}} by {{Sliding Attributes}}},
  url = {http://papers.nips.cc/paper/7178-fader-networksmanipulating-images-by-sliding-attributes.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  date = {2017},
  pages = {5967--5976},
  author = {Lample, Guillaume and Zeghidour, Neil and Usunier, Nicolas and Bordes, Antoine and DENOYER, Ludovic and Ranzato, Marc$\backslash$textquotesingle Aurelio},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  file = {/Users/sonynka/Zotero/storage/VZWRCWRC/Lample et al. - 2017 - Fader Networks Manipulating Images by Sliding Att.pdf;/Users/sonynka/Zotero/storage/HUE7PKZM/1706.html},
  annotation = {facebookresearch

~}
}

@article{jetchev_conditional_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.04695},
  primaryClass = {cs, stat},
  title = {The {{Conditional Analogy GAN}}: {{Swapping Fashion Articles}} on {{People Images}}},
  url = {http://arxiv.org/abs/1709.04695},
  shorttitle = {The {{Conditional Analogy GAN}}},
  abstract = {We present a novel method to solve image analogy problems : it allows to learn the relation between paired images present in training data, and then generalize and generate images that correspond to the relation, but were never seen in the training set. Therefore, we call the method Conditional Analogy Generative Adversarial Network (CAGAN), as it is based on adversarial training and employs deep convolutional neural networks. An especially interesting application of that technique is automatic swapping of clothing on fashion model photos. Our work has the following contributions. First, the definition of the end-to-end trainable CAGAN architecture, which implicitly learns segmentation masks without expensive supervised labeling data. Second, experimental results show plausible segmentation masks and often convincing swapped images, given the target article. Finally, we discuss the next steps for that technique: neural network architecture improvements and more advanced applications.},
  urldate = {2018-05-06},
  date = {2017-09-14},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Jetchev, Nikolay and Bergmann, Urs},
  file = {/Users/sonynka/Zotero/storage/URKHU8SB/Jetchev and Bergmann - 2017 - The Conditional Analogy GAN Swapping Fashion Arti.pdf;/Users/sonynka/Zotero/storage/WBBX74JL/1709.html},
  annotation = {Comment: To appear at the International Conference on Computer Vision, ICCV 2017, Workshop on Computer Vision for Fashion}
}

@article{zhu_be_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.07346},
  primaryClass = {cs},
  title = {Be {{Your Own Prada}}: {{Fashion Synthesis}} with {{Structural Coherence}}},
  url = {http://arxiv.org/abs/1710.07346},
  shorttitle = {Be {{Your Own Prada}}},
  abstract = {We present a novel and effective approach for generating new clothing on a wearer through generative adversarial learning. Given an input image of a person and a sentence describing a different outfit, our model "redresses" the person as desired, while at the same time keeping the wearer and her/his pose unchanged. Generating new outfits with precise regions conforming to a language description while retaining wearer's body structure is a new challenging task. Existing generative adversarial networks are not ideal in ensuring global coherence of structure given both the input photograph and language description as conditions. We address this challenge by decomposing the complex generative process into two conditional stages. In the first stage, we generate a plausible semantic segmentation map that obeys the wearer's pose as a latent spatial arrangement. An effective spatial constraint is formulated to guide the generation of this semantic segmentation map. In the second stage, a generative model with a newly proposed compositional mapping layer is used to render the final image with precise regions and textures conditioned on this map. We extended the DeepFashion dataset [8] by collecting sentence descriptions for 79K images. We demonstrate the effectiveness of our approach through both quantitative and qualitative evaluations. A user study is also conducted. The codes and the data are available at http://mmlab.ie.cuhk. edu.hk/projects/FashionGAN/.},
  urldate = {2018-05-06},
  date = {2017-10-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Zhu, Shizhan and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua and Loy, Chen Change},
  file = {/Users/sonynka/Zotero/storage/UW9SI5AP/Zhu et al. - 2017 - Be Your Own Prada Fashion Synthesis with Structur.pdf;/Users/sonynka/Zotero/storage/2GVEVVQF/1710.html},
  annotation = {Comment: This is the updated version of our original paper appeared in ICCV 2017 proceedings}
}

@article{zhang_self-attention_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08318},
  primaryClass = {cs, stat},
  title = {Self-{{Attention Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1805.08318},
  abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  urldate = {2018-05-30},
  date = {2018-05-21},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  file = {/Users/sonynka/Zotero/storage/UDI7Z5HP/Zhang et al. - 2018 - Self-Attention Generative Adversarial Networks.pdf;/Users/sonynka/Zotero/storage/W5XGNJ3U/1805.html}
}

@article{gulrajani_improved_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.00028},
  primaryClass = {cs, stat},
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  url = {http://arxiv.org/abs/1704.00028},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  urldate = {2018-06-14},
  date = {2017-03-31},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  file = {/Users/sonynka/Zotero/storage/EDFS32UC/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf;/Users/sonynka/Zotero/storage/3JBGT5VN/1704.html},
  annotation = {Comment: NIPS camera-ready}
}

@article{barnett_convergence_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.11382},
  primaryClass = {cs, stat},
  title = {Convergence {{Problems}} with {{Generative Adversarial Networks}} ({{GANs}})},
  url = {http://arxiv.org/abs/1806.11382},
  abstract = {Generative adversarial networks (GANs) are a novel approach to generative modelling, a task whose goal it is to learn a distribution of real data points. They have often proved difficult to train: GANs are unlike many techniques in machine learning, in that they are best described as a two-player game between a discriminator and generator. This has yielded both unreliability in the training process, and a general lack of understanding as to how GANs converge, and if so, to what. The purpose of this dissertation is to provide an account of the theory of GANs suitable for the mathematician, highlighting both positive and negative results. This involves identifying the problems when training GANs, and how topological and game-theoretic perspectives of GANs have contributed to our understanding and improved our techniques in recent years.},
  urldate = {2018-07-07},
  date = {2018-06-29},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Barnett, Samuel A.},
  file = {/Users/sonynka/Zotero/storage/V7KAW6SZ/Barnett - 2018 - Convergence Problems with Generative Adversarial N.pdf;/Users/sonynka/Zotero/storage/NNWM38TG/1806.html},
  annotation = {Comment: 47 pages, 4 figures}
}

@incollection{goodfellow_generative_2014,
  title = {Generative {{Adversarial Nets}}},
  url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  date = {2014},
  pages = {2672--2680},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.}
}

@online{giles_ganfather_nodate,
  langid = {english},
  title = {The {{GANfather}}: {{The}} Man Who’s given Machines the Gift of Imagination},
  url = {https://www.technologyreview.com/s/610253/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/},
  shorttitle = {The {{GANfather}}},
  abstract = {By pitting neural networks against one another, Ian Goodfellow has created a powerful AI tool. Now he, and the rest of us, must face the consequences.},
  journaltitle = {MIT Technology Review},
  urldate = {2018-07-22},
  author = {Giles, Martin},
  file = {/Users/sonynka/Zotero/storage/HH7CUGYR/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination.html}
}

@article{radford_unsupervised_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06434},
  primaryClass = {cs},
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1511.06434},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  urldate = {2018-07-22},
  date = {2015-11-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  file = {/Users/sonynka/Zotero/storage/SN3CU6HX/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf;/Users/sonynka/Zotero/storage/48ZI3J7L/1511.html},
  annotation = {Comment: Under review as a conference paper at ICLR 2016}
}

@article{isola_image--image_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.07004},
  primaryClass = {cs},
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  url = {http://arxiv.org/abs/1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  urldate = {2018-07-22},
  date = {2016-11-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  file = {/Users/sonynka/Zotero/storage/ET9UM9H7/Isola et al. - 2016 - Image-to-Image Translation with Conditional Advers.pdf;/Users/sonynka/Zotero/storage/DZIGDPG6/1611.html},
  annotation = {Comment: Website: https://phillipi.github.io/pix2pix/}
}

@online{_gan_2018,
  langid = {russian},
  title = {GAN paper list and review},
  url = {http://spark-in.me/post/gan-paper-review},
  abstract = {In this I list useful / influential GAN papers and papers related to sparse unsupervised data CNN training / latent space operations
Статьи автора - http://spark-in.me/author/snakers41
Блог - http://spark-in.me},
  journaltitle = {Spark in me},
  urldate = {2018-07-22},
  year = {2018-01-04T01:23:27.727119},
  author = {Вейсов, Александр},
  file = {/Users/sonynka/Zotero/storage/4XS9LET6/gan-paper-review.html}
}

@article{zhu_unpaired_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.10593},
  primaryClass = {cs},
  title = {Unpaired {{Image}}-to-{{Image Translation}} Using {{Cycle}}-{{Consistent Adversarial Networks}}},
  url = {http://arxiv.org/abs/1703.10593},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X $\backslash$rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y $\backslash$rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) $\backslash$approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  urldate = {2018-07-22},
  date = {2017-03-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  file = {/Users/sonynka/Zotero/storage/IGFIF5WG/Zhu et al. - 2017 - Unpaired Image-to-Image Translation using Cycle-Co.pdf;/Users/sonynka/Zotero/storage/D5Y228LY/1703.html},
  annotation = {Comment: An extended version of our ICCV 2017 paper, v4 updates the implementation details in the appendix}
}

@article{salimans_improved_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03498},
  primaryClass = {cs},
  title = {Improved {{Techniques}} for {{Training GANs}}},
  url = {http://arxiv.org/abs/1606.03498},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  urldate = {2018-07-22},
  date = {2016-06-10},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  file = {/Users/sonynka/Zotero/storage/J8SHLTP8/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf;/Users/sonynka/Zotero/storage/Y9GPSCD2/1606.html}
}

@article{yoo_pixel-level_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.07442},
  primaryClass = {cs},
  title = {Pixel-{{Level Domain Transfer}}},
  url = {http://arxiv.org/abs/1603.07442},
  abstract = {We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.},
  urldate = {2018-07-22},
  date = {2016-03-24},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence},
  author = {Yoo, Donggeun and Kim, Namil and Park, Sunggyun and Paek, Anthony S. and Kweon, In So},
  file = {/Users/sonynka/Zotero/storage/ASIUFWL5/Yoo et al. - 2016 - Pixel-Level Domain Transfer.pdf;/Users/sonynka/Zotero/storage/UU36UD4B/1603.html},
  annotation = {Comment: Published in ECCV 2016. Code and dataset available at dgyoo.github.io}
}

@article{arjovsky_towards_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.04862},
  primaryClass = {cs, stat},
  title = {Towards {{Principled Methods}} for {{Training Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1701.04862},
  abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
  urldate = {2018-07-28},
  date = {2017-01-17},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Arjovsky, Martin and Bottou, Léon},
  file = {/Users/sonynka/Zotero/storage/EKD5AWR5/Arjovsky and Bottou - 2017 - Towards Principled Methods for Training Generative.pdf;/Users/sonynka/Zotero/storage/S2IWIDNW/1701.html}
}

@online{hesse_image--image_2017,
  title = {Image-to-{{Image Translation}} in {{Tensorflow}} - {{Affine Layer}}},
  url = {https://affinelayer.com/pix2pix/},
  journaltitle = {Image-to-Image Translation in Tensorflow},
  urldate = {2018-07-28},
  date = {2017-01-25},
  author = {Hesse, Christopher},
  file = {/Users/sonynka/Zotero/storage/NH7D8TT6/pix2pix.html}
}

@article{mirza_conditional_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.1784},
  primaryClass = {cs, stat},
  title = {Conditional {{Generative Adversarial Nets}}},
  url = {http://arxiv.org/abs/1411.1784},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  urldate = {2018-07-28},
  date = {2014-11-06},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Mirza, Mehdi and Osindero, Simon},
  file = {/Users/sonynka/Zotero/storage/64LSPCFZ/Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf;/Users/sonynka/Zotero/storage/RKZMWAI7/1411.html}
}

@article{zhu_generative_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.03552},
  primaryClass = {cs},
  title = {Generative {{Visual Manipulation}} on the {{Natural Image Manifold}}},
  url = {http://arxiv.org/abs/1609.03552},
  abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
  urldate = {2018-07-28},
  date = {2016-09-12},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and Efros, Alexei A.},
  file = {/Users/sonynka/Zotero/storage/UISHLDB9/Zhu et al. - 2016 - Generative Visual Manipulation on the Natural Imag.pdf;/Users/sonynka/Zotero/storage/VY742C44/1609.html},
  annotation = {Comment: In European Conference on Computer Vision (ECCV 2016)}
}

@article{pathak_context_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.07379},
  primaryClass = {cs},
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  url = {http://arxiv.org/abs/1604.07379},
  shorttitle = {Context {{Encoders}}},
  abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
  urldate = {2018-07-28},
  date = {2016-04-25},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  file = {/Users/sonynka/Zotero/storage/AJT3A22A/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf;/Users/sonynka/Zotero/storage/RB9Y92II/1604.html},
  annotation = {Comment: New results on ImageNet Generation}
}

@online{noauthor_picsbuffet_nodate,
  title = {Picsbuffet},
  url = {https://picsbuffet.com/fotolia/#0,147,1576},
  urldate = {2018-07-29},
  file = {/Users/sonynka/Zotero/storage/TWFWA39L/fotolia.html}
}

@online{sonnenberg_akiwi_nodate,
  title = {Akiwi - a Keywording Tool},
  url = {http://www.akiwi.eu},
  abstract = {www.akiwi.eu - a web page suggesting keywords for uploaded images},
  journaltitle = {akiwi},
  urldate = {2018-07-29},
  author = {Sonnenberg, Prof Dr Kai-Uwe Barthel; Jonas Hartmann; Nico Hezel; Mike Krause; Anja},
  file = {/Users/sonynka/Zotero/storage/7RZVV6BB/www.akiwi.eu.html}
}

@online{zalando_damenmode_nodate,
  title = {Damenmode \& {{Damenschuhe}} Bei {{ZALANDO}} | {{Frauenmode}} Online Kaufen},
  url = {https://www.zalando.de/damen-home/},
  urldate = {2018-07-29},
  author = {{Zalando}},
  file = {/Users/sonynka/Zotero/storage/6SV8VYVM/damen-home.html}
}

@online{about_you_mode_nodate,
  title = {Mode Online von Mehr Als 1.000 {{Top}}-{{Marken}} | {{ABOUT YOU}}},
  url = {https://www.aboutyou.de/},
  urldate = {2018-07-29},
  author = {{ABOUT YOU}},
  file = {/Users/sonynka/Zotero/storage/PSSFYPUC/www.aboutyou.de.html}
}

@online{pc_damen_nodate,
  title = {Damen {{Sommermode}} von {{P}}\&{{C}}*: {{Die}} Neuesten {{Trends}} Für {{Frauen Online Shop}} | {{FASHION ID Online Shop}}},
  url = {https://www.fashionid.de/damen/},
  urldate = {2018-07-29},
  author = {{P\&C}},
  file = {/Users/sonynka/Zotero/storage/3LPWMYRG/damen.html}
}

@article{odena_conditional_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.09585},
  primaryClass = {cs, stat},
  title = {Conditional {{Image Synthesis With Auxiliary Classifier GANs}}},
  url = {http://arxiv.org/abs/1610.09585},
  abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  urldate = {2018-07-30},
  date = {2016-10-29},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  file = {/Users/sonynka/Zotero/storage/J2L7FTJN/Odena et al. - 2016 - Conditional Image Synthesis With Auxiliary Classif.pdf;/Users/sonynka/Zotero/storage/QIIK8SZR/1610.html}
}

@article{wang_high-resolution_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.11585},
  primaryClass = {cs},
  title = {High-{{Resolution Image Synthesis}} and {{Semantic Manipulation}} with {{Conditional GANs}}},
  url = {http://arxiv.org/abs/1711.11585},
  abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
  urldate = {2018-07-30},
  date = {2017-11-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
  file = {/Users/sonynka/Zotero/storage/XE8V6KF4/Wang et al. - 2017 - High-Resolution Image Synthesis and Semantic Manip.pdf;/Users/sonynka/Zotero/storage/YT4QSHFR/1711.html}
}

@online{noauthor_transfer_nodate,
  title = {Transfer {{Learning}} Tutorial — {{PyTorch Tutorials}} 0.4.1 Documentation},
  url = {https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py},
  urldate = {2018-08-10},
  file = {/Users/sonynka/Zotero/storage/SRCVJVNG/transfer_learning_tutorial.html}
}

@article{reinhard_color_2001,
  langid = {english},
  title = {Color {{Transfer}} between {{Images}}},
  journaltitle = {IEEE Computer Graphics and Applications},
  date = {2001},
  pages = {8},
  author = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
  file = {/Users/sonynka/Zotero/storage/IUJJ6TIM/Reinhard et al. - 2001 - Color Transfer between Images.pdf}
}

@online{lecun_mnist_nodate,
  title = {{{MNIST}} Handwritten Digit Database},
  url = {http://yann.lecun.com/exdb/mnist/},
  urldate = {2018-08-20},
  author = {LeCun, Yann and Cortes, Corinna and Burges, Chris},
  file = {/Users/sonynka/Zotero/storage/I3PPHJNX/mnist.html}
}

@software{kim_dcgan-tensorflow_2018,
  title = {{{DCGAN}}-Tensorflow: {{A}} Tensorflow Implementation of "{{Deep Convolutional Generative Adversarial Networks}}"},
  url = {https://github.com/carpedm20/DCGAN-tensorflow},
  shorttitle = {{{DCGAN}}-Tensorflow},
  urldate = {2018-08-20},
  date = {2018-08-20T09:24:17Z},
  keywords = {dcgan,gan,generative-model,tensorflow},
  author = {Kim, Taehoon},
  origdate = {2015-12-11T02:06:40Z}
}

@online{hesse_image--image_nodate,
  title = {Image-to-{{Image Demo}} - {{Affine Layer}}},
  url = {https://affinelayer.com/pixsrv/},
  journaltitle = {Image-to-Image Demo},
  urldate = {2018-08-20},
  author = {Hesse, Christopher},
  file = {/Users/sonynka/Zotero/storage/NJM29BCU/pixsrv.html}
}

@software{rkjones4_gangogh_2018,
  title = {{{GANGogh}}: {{Using GANs}} to Create {{Art}}},
  url = {https://github.com/rkjones4/GANGogh},
  shorttitle = {{{GANGogh}}},
  urldate = {2018-08-20},
  date = {2018-08-06T22:36:34Z},
  author = {{rkjones4}},
  origdate = {2017-06-18T01:47:11Z}
}

@article{karras_progressive_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.10196},
  primaryClass = {cs, stat},
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  url = {http://arxiv.org/abs/1710.10196},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024\^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  urldate = {2018-08-20},
  date = {2017-10-27},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  file = {/Users/sonynka/Zotero/storage/T4MP8R2P/Karras et al. - 2017 - Progressive Growing of GANs for Improved Quality, .pdf;/Users/sonynka/Zotero/storage/MFC2BNPA/1710.html},
  annotation = {Comment: Final ICLR 2018 version}
}

@article{lecun_convolutional_1995,
  title = {Convolutional {{Networks}} for {{Images}}, {{Speech}}, and {{Time}}-{{Series}}},
  journaltitle = {The Handbook of Brain Theory and Neural Networks},
  date = {1995},
  author = {LeCun, Y. and Bengio, Y.},
  editor = {Arbib, M. A.}
}

@article{gauthier_conditional_2015,
  title = {Conditional Generative Adversarial Nets for Convolutional Face Generation},
  date = {2015},
  author = {Gauthier, Jon}
}

@article{reed_generative_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.05396},
  primaryClass = {cs},
  title = {Generative {{Adversarial Text}} to {{Image Synthesis}}},
  url = {http://arxiv.org/abs/1605.05396},
  abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
  urldate = {2018-08-20},
  date = {2016-05-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  file = {/Users/sonynka/Zotero/storage/NES6U7TC/Reed et al. - 2016 - Generative Adversarial Text to Image Synthesis.pdf;/Users/sonynka/Zotero/storage/AKKFA4HZ/1605.html},
  annotation = {Comment: ICML 2016}
}

@article{springenberg_striving_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6806},
  primaryClass = {cs},
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  url = {http://arxiv.org/abs/1412.6806},
  shorttitle = {Striving for {{Simplicity}}},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  urldate = {2018-08-20},
  date = {2014-12-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  file = {/Users/sonynka/Zotero/storage/QBNWGLLL/Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net.pdf;/Users/sonynka/Zotero/storage/ICBLVD52/1412.html},
  annotation = {Comment: accepted to ICLR-2015 workshop track; no changes other than style}
}

@article{ioffe_batch_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03167},
  primaryClass = {cs},
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  url = {http://arxiv.org/abs/1502.03167},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  urldate = {2018-08-20},
  date = {2015-02-10},
  keywords = {Computer Science - Machine Learning},
  author = {Ioffe, Sergey and Szegedy, Christian},
  file = {/Users/sonynka/Zotero/storage/HMQ4LM4R/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/Users/sonynka/Zotero/storage/S6ZK256Q/1502.html}
}

@inproceedings{nair_rectified_2010,
  location = {{USA}},
  title = {Rectified {{Linear Units Improve Restricted Boltzmann Machines}}},
  isbn = {978-1-60558-907-7},
  url = {http://dl.acm.org/citation.cfm?id=3104322.3104425},
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  series = {ICML'10},
  publisher = {{Omnipress}},
  urldate = {2018-08-20},
  date = {2010},
  pages = {807--814},
  author = {Nair, Vinod and Hinton, Geoffrey E.}
}

@inproceedings{liu2016deepfashion,
  title = {{{DeepFashion}}: {{Powering Robust Clothes Recognition}} and {{Retrieval}} with {{Rich Annotations}}},
  url = {http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/AttributePrediction.html},
  booktitle = {Proceedings of {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  date = {2016-06},
  author = {Liu, Ziwei and Luo, Ping and Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou}
}

@article{ronneberger_u-net_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.04597},
  primaryClass = {cs},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  url = {http://arxiv.org/abs/1505.04597},
  shorttitle = {U-{{Net}}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  urldate = {2018-08-22},
  date = {2015-05-18},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  file = {/Users/sonynka/Zotero/storage/GKR5NTA7/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;/Users/sonynka/Zotero/storage/3YJZL5PL/1505.html},
  annotation = {Comment: conditionally accepted at MICCAI 2015}
}

@online{hesse_image--image_nodate-1,
  title = {Image-to-{{Image Translation}} in {{Tensorflow}} - {{Affine Layer}}},
  url = {https://affinelayer.com/pix2pix/},
  journaltitle = {Image-to-Image Translation in Tensorflow},
  urldate = {2018-08-22},
  author = {Hesse, Christopher},
  file = {/Users/sonynka/Zotero/storage/G4U4L4MM/pix2pix.html}
}

@article{johnson_perceptual_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.08155},
  primaryClass = {cs},
  title = {Perceptual {{Losses}} for {{Real}}-{{Time Style Transfer}} and {{Super}}-{{Resolution}}},
  url = {http://arxiv.org/abs/1603.08155},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a $\backslash$emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing $\backslash$emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  urldate = {2018-08-22},
  date = {2016-03-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  file = {/Users/sonynka/Zotero/storage/BHZRZHEG/Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf;/Users/sonynka/Zotero/storage/RREDFPLQ/1603.html}
}

@article{li_precomputed_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.04382},
  primaryClass = {cs},
  title = {Precomputed {{Real}}-{{Time Texture Synthesis}} with {{Markovian Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1604.04382},
  abstract = {This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative neural networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feed-forward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required any longer at generation time, our run-time performance (0.25M pixel images at 25Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization.},
  urldate = {2018-08-22},
  date = {2016-04-15},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Li, Chuan and Wand, Michael},
  file = {/Users/sonynka/Zotero/storage/6VHA226H/Li and Wand - 2016 - Precomputed Real-Time Texture Synthesis with Marko.pdf;/Users/sonynka/Zotero/storage/6F39FIN9/1604.html},
  annotation = {Comment: 17 pages, 15 figures}
}

@inproceedings{pmlr-v70-arjovsky17a,
  location = {{International Convention Centre, Sydney, Australia}},
  title = {Wasserstein {{Generative Adversarial Networks}}},
  volume = {70},
  url = {http://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  series = {Proceedings of Machine Learning Research},
  publisher = {{PMLR}},
  date = {2017-08-06/2017-08-11},
  pages = {214-223},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  editor = {Precup, Doina and Teh, Yee Whye}
}

@inreference{noauthor_k-means_2018,
  langid = {english},
  title = {K-Means Clustering},
  url = {https://en.wikipedia.org/w/index.php?title=K-means_clustering&oldid=855299299},
  abstract = {k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.
The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.
The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means due to the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.},
  booktitle = {Wikipedia},
  urldate = {2018-08-22},
  date = {2018-08-17T08:56:06Z},
  file = {/Users/sonynka/Zotero/storage/CV9IATIH/index.html},
  note = {Page Version ID: 855299299}
}

@online{noauthor_welcome_nodate,
  langid = {english},
  title = {Welcome to {{Python}}.Org},
  url = {https://www.python.org/},
  abstract = {The official home of the Python Programming Language},
  journaltitle = {Python.org},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/NTG5KZPT/www.python.org.html}
}

@online{noauthor_pytorch_nodate,
  langid = {english},
  title = {{{PyTorch}}},
  url = {https://pytorch.org/},
  abstract = {Tensors and Dynamic neural networks in Python
                    with strong GPU acceleration.},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/2M5T5NBN/pytorch.org.html}
}

@online{noauthor_python_nodate,
  title = {Python {{Data Analysis Library}} — Pandas: {{Python Data Analysis Library}}},
  url = {https://pandas.pydata.org/},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/D3ZTMJJC/pandas.pydata.org.html}
}

@online{noauthor_numpy_nodate,
  title = {{{NumPy}} — {{NumPy}}},
  url = {http://www.numpy.org/},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/GGCTZFKU/www.numpy.org.html}
}

@online{noauthor_scikit-learn_nodate,
  title = {Scikit-Learn: Machine Learning in {{Python}} — Scikit-Learn 0.19.2 Documentation},
  url = {http://scikit-learn.org/stable/},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/89NQNC74/stable.html}
}

@online{noauthor_matplotlib_nodate,
  title = {Matplotlib: {{Python}} Plotting — {{Matplotlib}} 2.2.3 Documentation},
  url = {https://matplotlib.org/},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/IHWF49F3/matplotlib.org.html}
}

@online{noauthor_pillow_nodate,
  title = {Pillow — {{Pillow}} ({{PIL Fork}}) 5.3.0.Dev0 Documentation},
  url = {https://pillow.readthedocs.io/en/latest/#},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/PY5A3SQV/latest.html}
}

@online{noauthor_sklearn.cluster.kmeans_nodate,
  title = {Sklearn.Cluster.{{KMeans}} — Scikit-Learn 0.19.2 Documentation},
  url = {http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html},
  urldate = {2018-08-22},
  file = {/Users/sonynka/Zotero/storage/WJD2LAB7/sklearn.cluster.KMeans.html}
}

@software{noauthor_junyanz/pytorch-cyclegan-and-pix2pix_nodate,
  title = {Junyanz/Pytorch-{{CycleGAN}}-and-Pix2pix: {{Image}}-to-Image Translation in {{PyTorch}} (e.g., Horse2zebra, Edges2cats, and More)},
  url = {https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix},
  urldate = {2018-08-25},
  file = {/Users/sonynka/Zotero/storage/TXCQKNZT/pytorch-CycleGAN-and-pix2pix.html}
}


