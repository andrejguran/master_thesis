
@article{karras_progressive_2017,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	urldate = {2018-08-20TZ},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10196},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@misc{rkjones4_gangogh:_2018,
	title = {{GANGogh}: {Using} {GANs} to create {Art}},
	shorttitle = {{GANGogh}},
	url = {https://github.com/rkjones4/GANGogh},
	urldate = {2018-08-20TZ},
	author = {rkjones4},
	month = aug,
	year = {2018},
	note = {original-date: 2017-06-18T01:47:11Z}
}

@misc{hesse_image--image_nodate,
	title = {Image-to-{Image} {Demo} - {Affine} {Layer}},
	url = {https://affinelayer.com/pixsrv/},
	urldate = {2018-08-20TZ},
	journal = {Image-to-Image Demo},
	author = {Hesse, Christopher}
}

@misc{kim_dcgan-tensorflow:_2018,
	title = {{DCGAN}-tensorflow: {A} tensorflow implementation of "{Deep} {Convolutional} {Generative} {Adversarial} {Networks}"},
	copyright = {MIT},
	shorttitle = {{DCGAN}-tensorflow},
	url = {https://github.com/carpedm20/DCGAN-tensorflow},
	urldate = {2018-08-20TZ},
	author = {Kim, Taehoon},
	month = aug,
	year = {2018},
	note = {original-date: 2015-12-11T02:06:40Z},
	keywords = {dcgan, gan, generative-model, tensorflow}
}

@misc{lecun_mnist_nodate,
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2018-08-20TZ},
	author = {LeCun, Yann and Cortes, Corinna and Burges, Chris}
}

@article{reinhard_color_2001,
	title = {Color {Transfer} between {Images}},
	language = {en},
	journal = {IEEE Computer Graphics and Applications},
	author = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
	year = {2001},
	pages = {8}
}

@misc{noauthor_transfer_nodate,
	title = {Transfer {Learning} tutorial — {PyTorch} {Tutorials} 0.4.1 documentation},
	url = {https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py},
	urldate = {2018-08-10TZ}
}

@article{wang_high-resolution_2017,
	title = {High-{Resolution} {Image} {Synthesis} and {Semantic} {Manipulation} with {Conditional} {GANs}},
	url = {http://arxiv.org/abs/1711.11585},
	abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
	urldate = {2018-07-30TZ},
	journal = {arXiv:1711.11585 [cs]},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11585},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning}
}

@article{odena_conditional_2016,
	title = {Conditional {Image} {Synthesis} {With} {Auxiliary} {Classifier} {GANs}},
	url = {http://arxiv.org/abs/1610.09585},
	abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
	urldate = {2018-07-30TZ},
	journal = {arXiv:1610.09585 [cs, stat]},
	author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09585},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning}
}

@misc{about_you_mode_nodate,
	title = {Mode online von mehr als 1.000 {Top}-{Marken} {\textbar} {ABOUT} {YOU}},
	url = {https://www.aboutyou.de/},
	urldate = {2018-07-29TZ},
	author = {{ABOUT YOU}}
}

@misc{p&c_damen_nodate,
	title = {Damen {Sommermode} von {P}\&{C}*: {Die} neuesten {Trends} für {Frauen} {Online} {Shop} {\textbar} {FASHION} {ID} {Online} {Shop}},
	url = {https://www.fashionid.de/damen/},
	urldate = {2018-07-29TZ},
	author = {{P\&C}}
}

@misc{zalando_damenmode_nodate,
	title = {Damenmode \& {Damenschuhe} bei {ZALANDO} {\textbar} {Frauenmode} online kaufen},
	url = {https://www.zalando.de/damen-home/},
	urldate = {2018-07-29TZ},
	author = {{Zalando}}
}

@misc{sonnenberg_akiwi_nodate,
	title = {akiwi - a keywording tool},
	url = {http://www.akiwi.eu},
	abstract = {www.akiwi.eu - a web page suggesting keywords for uploaded images},
	urldate = {2018-07-29TZ},
	journal = {akiwi},
	author = {Sonnenberg, Prof Dr Kai-Uwe Barthel; Jonas Hartmann; Nico Hezel; Mike Krause; Anja}
}

@misc{noauthor_picsbuffet_nodate,
	title = {picsbuffet},
	url = {https://picsbuffet.com/fotolia/#0,147,1576},
	urldate = {2018-07-29TZ}
}

@misc{the_chinese_university_of_hong_kong_deepfashion_nodate,
	title = {{DeepFashion}},
	url = {http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html},
	urldate = {2018-07-29TZ},
	journal = {Large-scale Fashion (DeepFashion) Database},
	author = {The Chinese University of Hong Kong}
}

@article{pathak_context_2016,
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	shorttitle = {Context {Encoders}},
	url = {http://arxiv.org/abs/1604.07379},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	urldate = {2018-07-28TZ},
	journal = {arXiv:1604.07379 [cs]},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.07379},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning}
}

@article{zhu_generative_2016,
	title = {Generative {Visual} {Manipulation} on the {Natural} {Image} {Manifold}},
	url = {http://arxiv.org/abs/1609.03552},
	abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
	urldate = {2018-07-28TZ},
	journal = {arXiv:1609.03552 [cs]},
	author = {Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and Efros, Alexei A.},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03552},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2018-07-28TZ},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{hesse_image--image_2017,
	title = {Image-to-{Image} {Translation} in {Tensorflow} - {Affine} {Layer}},
	url = {https://affinelayer.com/pix2pix/},
	urldate = {2018-07-28TZ},
	journal = {Image-to-Image Translation in Tensorflow},
	author = {Hesse, Christopher},
	month = jan,
	year = {2017}
}

@article{arjovsky_towards_2017,
	title = {Towards {Principled} {Methods} for {Training} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1701.04862},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	urldate = {2018-07-28TZ},
	journal = {arXiv:1701.04862 [cs, stat]},
	author = {Arjovsky, Martin and Bottou, Léon},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04862},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{yoo_pixel-level_2016,
	title = {Pixel-{Level} {Domain} {Transfer}},
	url = {http://arxiv.org/abs/1603.07442},
	abstract = {We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.},
	urldate = {2018-07-22TZ},
	journal = {arXiv:1603.07442 [cs]},
	author = {Yoo, Donggeun and Kim, Namil and Park, Sunggyun and Paek, Anthony S. and Kweon, In So},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.07442},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition}
}

@article{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2018-07-22TZ},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{zhu_unpaired_2017,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2018-07-22TZ},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{_gan_2018,
	title = {{GAN} paper list and review},
	url = {http://spark-in.me/post/gan-paper-review},
	abstract = {In this I list useful / influential GAN papers and papers related to sparse unsupervised data CNN training / latent space operations
Статьи автора - http://spark-in.me/author/snakers41
Блог - http://spark-in.me},
	language = {ru},
	urldate = {2018-07-22TZ},
	journal = {Spark in me},
	author = {Вейсов, Александр},
	month = jan,
	year = {2018}
}

@article{isola_image--image_2016,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2018-07-22TZ},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{radford_unsupervised_2015,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2018-07-22TZ},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@misc{giles_ganfather:_nodate,
	title = {The {GANfather}: {The} man who’s given machines the gift of imagination},
	shorttitle = {The {GANfather}},
	url = {https://www.technologyreview.com/s/610253/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/},
	abstract = {By pitting neural networks against one another, Ian Goodfellow has created a powerful AI tool. Now he, and the rest of us, must face the consequences.},
	language = {en},
	urldate = {2018-07-22TZ},
	journal = {MIT Technology Review},
	author = {Giles, Martin}
}

@incollection{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680}
}

@article{barnett_convergence_2018,
	title = {Convergence {Problems} with {Generative} {Adversarial} {Networks} ({GANs})},
	url = {http://arxiv.org/abs/1806.11382},
	abstract = {Generative adversarial networks (GANs) are a novel approach to generative modelling, a task whose goal it is to learn a distribution of real data points. They have often proved difficult to train: GANs are unlike many techniques in machine learning, in that they are best described as a two-player game between a discriminator and generator. This has yielded both unreliability in the training process, and a general lack of understanding as to how GANs converge, and if so, to what. The purpose of this dissertation is to provide an account of the theory of GANs suitable for the mathematician, highlighting both positive and negative results. This involves identifying the problems when training GANs, and how topological and game-theoretic perspectives of GANs have contributed to our understanding and improved our techniques in recent years.},
	urldate = {2018-07-07TZ},
	journal = {arXiv:1806.11382 [cs, stat]},
	author = {Barnett, Samuel A.},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.11382},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2018-06-14TZ},
	journal = {arXiv:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = mar,
	year = {2017},
	note = {arXiv: 1704.00028},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{zhang_self-attention_2018,
	title = {Self-{Attention} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1805.08318},
	abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
	urldate = {2018-05-30TZ},
	journal = {arXiv:1805.08318 [cs, stat]},
	author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08318},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{zhu_be_2017,
	title = {Be {Your} {Own} {Prada}: {Fashion} {Synthesis} with {Structural} {Coherence}},
	shorttitle = {Be {Your} {Own} {Prada}},
	url = {http://arxiv.org/abs/1710.07346},
	abstract = {We present a novel and effective approach for generating new clothing on a wearer through generative adversarial learning. Given an input image of a person and a sentence describing a different outfit, our model "redresses" the person as desired, while at the same time keeping the wearer and her/his pose unchanged. Generating new outfits with precise regions conforming to a language description while retaining wearer's body structure is a new challenging task. Existing generative adversarial networks are not ideal in ensuring global coherence of structure given both the input photograph and language description as conditions. We address this challenge by decomposing the complex generative process into two conditional stages. In the first stage, we generate a plausible semantic segmentation map that obeys the wearer's pose as a latent spatial arrangement. An effective spatial constraint is formulated to guide the generation of this semantic segmentation map. In the second stage, a generative model with a newly proposed compositional mapping layer is used to render the final image with precise regions and textures conditioned on this map. We extended the DeepFashion dataset [8] by collecting sentence descriptions for 79K images. We demonstrate the effectiveness of our approach through both quantitative and qualitative evaluations. A user study is also conducted. The codes and the data are available at http://mmlab.ie.cuhk. edu.hk/projects/FashionGAN/.},
	urldate = {2018-05-06TZ},
	journal = {arXiv:1710.07346 [cs]},
	author = {Zhu, Shizhan and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua and Loy, Chen Change},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.07346},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{jetchev_conditional_2017,
	title = {The {Conditional} {Analogy} {GAN}: {Swapping} {Fashion} {Articles} on {People} {Images}},
	shorttitle = {The {Conditional} {Analogy} {GAN}},
	url = {http://arxiv.org/abs/1709.04695},
	abstract = {We present a novel method to solve image analogy problems : it allows to learn the relation between paired images present in training data, and then generalize and generate images that correspond to the relation, but were never seen in the training set. Therefore, we call the method Conditional Analogy Generative Adversarial Network (CAGAN), as it is based on adversarial training and employs deep convolutional neural networks. An especially interesting application of that technique is automatic swapping of clothing on fashion model photos. Our work has the following contributions. First, the definition of the end-to-end trainable CAGAN architecture, which implicitly learns segmentation masks without expensive supervised labeling data. Second, experimental results show plausible segmentation masks and often convincing swapped images, given the target article. Finally, we discuss the next steps for that technique: neural network architecture improvements and more advanced applications.},
	urldate = {2018-05-06TZ},
	journal = {arXiv:1709.04695 [cs, stat]},
	author = {Jetchev, Nikolay and Bergmann, Urs},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04695},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning}
}

@incollection{lample_fader_2017,
	title = {Fader {Networks}:{Manipulating} {Images} by {Sliding} {Attributes}},
	url = {http://papers.nips.cc/paper/7178-fader-networksmanipulating-images-by-sliding-attributes.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Lample, Guillaume and Zeghidour, Neil and Usunier, Nicolas and Bordes, Antoine and DENOYER, Ludovic and Ranzato, Marc{\textbackslash}textquotesingle Aurelio},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {5967--5976}
}

@article{choi_stargan:_2017,
	title = {{StarGAN}: {Unified} {Generative} {Adversarial} {Networks} for {Multi}-{Domain} {Image}-to-{Image} {Translation}},
	shorttitle = {{StarGAN}},
	url = {http://arxiv.org/abs/1711.09020},
	abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
	urldate = {2018-04-30TZ},
	journal = {arXiv:1711.09020 [cs]},
	author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{zhu_toward_2017,
	title = {Toward {Multimodal} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/1711.11586},
	abstract = {Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a {\textbackslash}emph\{distribution\} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.},
	urldate = {2018-04-30TZ},
	journal = {arXiv:1711.11586 [cs, stat]},
	author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A. and Wang, Oliver and Shechtman, Eli},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11586},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Statistics - Machine Learning}
}
@article{huang_multimodal_2018,
	title = {Multimodal {Unsupervised} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/1804.04732},
	abstract = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT.},
	urldate = {2018-04-30TZ},
	journal = {arXiv:1804.04732 [cs, stat]},
	author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.04732},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning}
}